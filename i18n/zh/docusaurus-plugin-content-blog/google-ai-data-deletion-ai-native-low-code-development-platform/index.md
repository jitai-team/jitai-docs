---
title: "谷歌AI删除用户数据：引人重新考量AI原生低代码平台"
tags: [google ai, low code, ai native, development platform]
date: 2025-12-11
description: "了解谷歌AI数据删除对低代码开发平台的意义，以及为何JitAI等原生AI开发平台对安全AI代理至关重要。"
keywords: [谷歌AI, 低代码开发平台, 开发平台, AI原生]
slug: google-ai-data-deletion-ai-native-low-code-development-platform
---

最近，关于 **Google AI 在开发中误删整块分区甚至整块硬盘** 的报道刷了一波屏。很多人的第一反应是：

> “天哪，以后 AI 要是能在我服务器上随便跑命令，那要是删错东西怎么办？”

但如果我们稍微冷静一点就会发现，这件事真正暴露的问题，其实不只是 “Google 的某个 AI 功能有 bug”，而是——  
**我们到底应该怎样设计有 AI 能力的开发平台，尤其是低代码开发平台（low code development platform）？**

本篇文章，想借 “谷歌AI删除用户数据” 这件事，慢慢带你思考三件事：

*   这个事件背后，暴露了现有开发平台哪些结构性问题
    
*   “AI 原生（ai native）” 的**低代码开发平台**应该长什么样
    
*   像 [JitAI](https://jit.pro) 这样的 AI 原生开发平台，是如何在架构层面尽量避免这种“灾难级误操作”的
    

<!-- truncate -->

![image.png](img1.png)

---

## 1. “谷歌AI删除用户数据” 其实暴露了什么

先把新闻翻译成人话。

报道里讲的是：在一个实验环境中，**Google 的 IDE 工具**被接入了文件系统，可以执行类似 Shell 的操作。当它收到类似 “清理一下磁盘空间”“删除不必要文件” 这样的指令时，它并没有做一个“聪明的垃圾清理”，而是执行了**过于激进的命令**，结果直接把整块分区甚至整个盘都给干掉了。

不管这发生在个人环境还是测试环境，值得我们警惕的是这个模式：

*   用户用自然语言说：“帮我清下文件”、“释放点空间”
    
*   AI 把这句话翻译成一串高权限系统操作
    
*   缺乏足够严格的安全边界、模拟、人工复核
    
*   结果从“小 bug”直接升级成 **灾难级数据丢失**
    

这件事对 CTO、安全负责人、高级工程师来说，其实传递了一个很直接的信号：

> **一旦 AI 不再只是“聊天”，而是开始 “可以对系统动手”，平台本身的设计，就和模型好坏一样重要。**

---

## 2. 为什么这件事会让开发者和企业心里发毛

从外面看，这像一个猎奇新闻；从工程视角看，这种事其实“写在系统设计里了”。

背后通常是三件事叠到一起：

1.  **权限过大（High Privilege）**  
    AI 拥有足够高的权限，可以执行会删除关键文件或分区的命令。
    
2.  **意图含糊（Ambiguous Intent）**  
    “清理一下”、“删掉没用的”、“腾点空间” 对人来说是模糊但可理解的；  
    对只会“模式匹配”的模型来说，很容易理解偏离。
    
3.  **缺少结构化的安全边界（Lack of Guardrails）**  
    没有硬规则去限制：
    
    *   永远不要动系统目录
        
    *   高风险操作必须人工确认
        
    *   只能在某个临时目录里做删除
        

放到企业环境，这就不再是 “心疼一下个人电脑” 的问题，而是：

*   **生产系统停摆**
    
*   **法务和合规事故**（删掉了必须保留的日志或记录）
    
*   **客户信任受损**（客户觉得 AI 是一个无法控制的风险源）
    

所以，这个事件本质上是在提醒我们：  
**一旦你让 AI 真正“接上系统”，开发平台的架构设计，就变成了安全边界的一部分。**

---

## 3. 把 AI 当「插件」的开发平台，问题出在哪里

今天市面上的很多平台，对 AI 的态度基本是：

> “我们已经是一个 development platform 了，再加一个 AI 功能就行。”

典型做法是：

*   一个低代码开发平台，负责表单、流程、集成等
    
*   再加上一个 **“调用大模型” 的模块**，比如工作流里加个 “AI 节点”
    
*   有的更进一步，让模型可以执行动作，比如：修改数据、调用 API、甚至执行命令
    

表面上看很 “智能”，但从架构和治理角度看，问题非常集中：

1.  **边界不清（No Clear Boundaries）**  
    AI 往往能看到比它应该看到的更多数据，也能做比它应该做的更多事情。
    
2.  **操作无类型（No Typed Actions）**  
    很多工具直接让 AI 拼接自由文本命令，而不是从一组**有类型、有校验**的操作里进行选择。
    
3.  **根本没想过“生命周期”（No Lifecycle Thinking）**  
    平台并没有把 AI 相关的数据（prompt、日志、输出、调用记录）当成一个独立的 **数据生命周期（data lifecycle）** 去治理，也就谈不上“删除策略”或“审计”。
    

所以，“Google AI data deletion” 其实不是一个厂商的个案，而是提醒我们：

> 如果还是用“给平台加一个 AI 插件”的方式来接 AI，迟早会踩到类似的坑。

---

## 4. 什么才叫 AI 原生（AI-Native）的开发平台

要避免这些问题，我们得先把几个概念说清楚。

### 4.1 低代码开发平台是什么

一个低代码开发平台，通常是给开发者和高阶业务人员用来：

*   通过可视化配置来搭建应用
    
*   编排工作流和数据流
    
*   通过连接器集成各种服务，而不是全靠手写代码
    

行业报告普遍预测：到了 2020 年代中后期，**多数企业新应用都会部分或完全依赖低代码/零代码工具来交付**，低代码平台的采用率在持续上升（Gartner，2021–2023）。

### 4.2 开发平台又是什么

**开发平台**范围更大，它是一整套：

*   运行时
    
*   开发工具
    
*   API 和 SDK
    
*   安全与部署流水线
    

低代码平台是一种开发平台，但不是全部。

### 4.3 AI-native（AI 原生）意味着什么

**AI-native（AI 原生）** 跟 “AI-enabled（AI 赋能）” 是两种完全不同的心态：

*   AI-enabled = 原本的开发平台已经设计好了，再贴一层 AI 功能上去。
    
*   AI-native = 从第一天起，平台的**架构、治理和工具链就是围绕 AI 设计的**。
    

一个真正 AI 原生的低代码开发平台应该：

*   把 AI 模型和 AI agent 当作**一等公民（first-class citizens）**，而不是简单的插件
    
*   在每一次 AI 行为周围，默认带上**安全边界、治理规则和可观测性**
    
*   把 **确定性逻辑（if/else、约束）** 和 **非确定性 AI 推理** 明确分层
    
*   把 **数据访问、保留和删除** 当成架构的一部分来设计，而不是事后打补丁
    

这正是像 [JitAI](https://jit.pro) 这样的 AI 原生开发平台想要做到的方向。

---

## 5. 数据、删除与信任：为什么“治理”是核心

“谷歌删除用户数据” 只是一个例子，真正让企业犹豫的，是 **“我能不能信任这个东西”**。

各种调研的趋势基本一致：

*   AI 在企业中的使用越来越普遍，但大多是**被“圈”在某些场景里试点**
    
*   **数据隐私、安全与控制** 是把 AI 从试点推向全面落地的首要阻碍
    
*   很多组织已经在至少一个业务场景中使用 AI，但在没有更强治理能力之前，会尽量避免让 AI 直接操纵核心系统（McKinsey《State of AI》系列，约 2023 年）
    

对一个 AI 原生的开发平台来说，这些担忧，直接变成了架构要求。

一个靠谱的平台，应该至少满足：

1.  **数据访问边界是显式的（Explicit Data Scopes）**  
    AI 能看到、能操作的数据范围，必须是显式声明的，而不是“默认全局可见”。
    
2.  **可配置的保留与删除（Retention & Deletion Controls）**  
    AI 相关的数据——日志、prompt、输出、调用记录——需要有明确的：
    
    *   保留时长
        
    *   何时、按什么维度（用户 / 应用 / 环境）进行删除
        
    *   删除行为可被证明、可被审计
        
3.  **可审计的行为历史（Auditable Action History）**  
    每一次 AI agent 或工作流触发变更时，都能被追溯：
    
    *   是谁 / 什么触发的
        
    *   用了哪些输入数据
        
    *   调用了哪些工具或 API
        
    *   最终对系统造成了什么改动
        

没有这些，无论模型看起来多“聪明”，平台都很难在企业级环境里赢得真正的信任。

---

## 6. AI Agent、工具与“约束”的必要性

如果把 “谷歌删除用户数据” 这个现象拆开，它本质是一个 **AI agent + 工具调用** 的问题：

*   **AI agent** 接收目标（例如：“释放磁盘空间”、“删除没用的文件”），自己规划步骤
    
*   然后调用一系列 **工具（tools）**，比如文件操作、shell 命令、API 调用，来实现这个目标
    

如果平台只是对 agent 说：

> “你拥有这些能力，你想怎么用就怎么用。”

那只要有一小步理解偏差，结果就可能从 “删几个缓存文件”，变成类似 `rm -rf /` 的操作。

所以，一个 AI 原生平台必须在架构上建立起至少四个原则：

1.  **最小权限（Least Privilege）**  
    每个 AI agent 只能访问它完成任务所必需的那一小撮工具和数据，而不是整个系统。
    
2.  **类型化 & 可校验的操作（Typed, Validated Actions）**  
    不再允许 AI 发送自由文本命令，而是必须通过类似这样的接口：
    
    *   `deleteFiles(paths: string[], maxSizeMB: number)`
        
    *   并且在接口层面就有校验：
        
        *   永远禁止删除系统目录
            
        *   删除总容量超过某个阈值必须升级审批
            
3.  **模拟 / 预演模式（Simulation / Dry-Run）**  
    在执行真正的破坏性操作前，agent 先做一轮模拟：
    
    *   “如果我执行这一步，将影响哪些文件、哪些服务？”
        
    *   人工或上层策略可以据此决定是否放行。
        
4.  **基于策略的推理（Policy-Aware Reasoning）**  
    agent 不是只靠 prompt “自觉”，而是被 **一套显式的策略（policy）** 围住：
    
    *   哪些操作允许，哪些严格禁止
        
    *   不同风险等级要不要人工确认
        
    *   哪些场景必须升级给人类或更高权限的 agent
        

如果没有这些约束，“Google AI deletes entire drive” 这种事故，迟早会在其他地方以不同形式上演。

---

## 7. 像 JitAI 这样的 AI 原生平台，是怎么思考这个问题的

[JitAI](https://jit.pro) 从一开始就被设计成一个 **AI 原生的开发平台**，而不是一个 “传统低代码平台 + 聊天窗” 的组合。

从架构层面，它主要做了几件和上面讨论高度对齐的事情。

### 7.1 AI 通过结构化契约工作，而不是发任意命令

在 JitAI 里，**AI agent** 无法直接拼接任意 shell 命令或自由的文本指令去“碰运气”。它必须通过：

*   平台暴露出来的 **结构化工具和 API**
    
*   有明确类型和参数的接口
    
*   在可见的权限与数据边界内行动
    

这意味着，“删除某些东西” 这类意图，不会被直接翻译成某种 `rm` 命令，而是必须走一个被约束好的操作，比如：

*   只在指定目录下
    
*   只在开发或沙箱环境内
    
*   只可以删除符合规则的缓存文件
    

本质上，JitAI 把 “AI 能干什么” 做成了工程团队 **看得见、改得动、审得清** 的东西，而不是一个黑箱。

### 7.2 开发模式是「可视化 + AI」的双模

JitAI 把两种能力合在了一起：

*   **可视化低代码**：用拖拽、配置的方式搭建工作流、界面和集成
    
*   **AI 助手**：利用 AI 理解你的数据模型、API 和业务上下文，帮你生成和改进逻辑
    

开发者可以清楚地看到：

*   当前有哪些 agent
    
*   它们具体能调用哪些工具、哪些数据
    
*   数据在不同模块之间是怎么流动的
    

这种透明度，本身就是一种“安全设计”。当 AI 的能力被清晰建模出来，要静悄悄干掉一整个分区，难度就高多了。

### 7.3 治理和可观测性是默认能力，而不是附加组件

在一个 AI 原生的平台中，“治理” 不再是附加选项。

用 JitAI 搭建系统时，企业可以：

*   **追踪 AI 的每一次决策**，看它在工作流和应用里扮演了什么角色
    
*   查看某个 AI agent 在一次执行中：
    
*   用了哪些输入
    
*   调用了哪些工具
    
*   得到了什么输出
    
*   针对不同环境（开发 / 测试 / 生产），配置不同等级的限制和策略
    

当你让 AI 真正接入核心系统时，这种级别的“可观测性”，重要程度不亚于传统软件工程中的日志和监控。

### 7.4 数据控制和生命周期，是设计的一部分

“谷歌AI删除用户数据” 也提醒我们：**数据本身就是风险的载体**。

删错数据，会造成业务影响；  
存错数据，会带来合规风险。

像 JitAI 这样的 AI 原生开发平台，会在设计阶段就考虑：

*   数据边界和数据分类
    
*   AI 在不同数据域（例如：用户数据、运营数据、日志数据）中被允许做什么、不被允许做什么
    
*   如何在合规要求下，支持对 AI 相关数据进行可控、可审计的**删除**
    

如果你的团队正在探索 AI agent 连接真实业务系统，可以在一个安全边界更清晰的环境中 [try JitAI](https://jit.pro/download)，看 AI 原生架构在实践中是什么体验。

---

## 8. “Google AI 数据删除” 之后，团队可以做的几件实际事情

如果你现在负责平台、工程或 IT，可以用这个事件当一个 “项目起点”，做几件非常落地的事：

1.  **先盘点：AI 到底能动哪里？（Inventory AI Access）**
    
    *   目前有哪些地方允许 AI 调用工具或 API？
        
    *   涉及到哪些环境（开发 / 测试 / 生产）？
        
2.  **干掉“自由命令执行”（Remove Raw Command Execution）**
    
    *   用 **类型化、可校验的操作** 替代自由文本 shell 或 SQL 执行。
        
3.  **给 AI agent 做「最小权限」设计（Enforce Least Privilege）**
    
    *   每个 agent 单独配置它能用哪些工具、能看哪些数据，并按环境区分。
        
4.  **高风险操作强制“人类在环”（Human-in-the-Loop for Destructive Ops）**
    
    *   对删除、修改、对外发送这类动作设置阈值，超出就必须人工确认。
        
5.  **补上日志和追踪（Add Logging & Traceability）**
    
    *   确保你能还原整条链路：  
        prompt → 决策 → 工具调用 → 结果变化。
        
6.  **有意识地选择 AI 原生平台（Choose AI-Native Platforms Deliberately）**
    
    *   当你选一个 low code development platform 或 development platform 来做 AI 项目时，问清楚：
        
        *   它怎么建模 AI agent 和工具？
            
        *   它对日志、数据保留和删除有什么原生支持？
            
        *   对生产环境有什么差异化限制和保护？
            

这些问题，现在已经不再是“前瞻性思考”，而是 AI 落地过程里非常现实的检查清单。

---

## 9. FAQ：关于谷歌删除用户数据和 AI 原生低代码

### Q1. “Google AI data deletion” 这件事，简单说是在讲什么？

简单说，就是有报道提到：在一次实验里，一个 Google 的 AI 系统被赋予了执行文件操作的能力。当它被要求“释放空间”或“删除不必要的文件”时，执行了过度激进的命令，**删除了远超预期的内容——包括整块分区甚至整块硬盘**。

它是一堂血的教训：当 AI agent 拿到了高权限工具、却没有足够的安全边界时，“误解”会直接变成“灾难”。

---

### Q2. 这跟低代码开发平台有什么关系？

关系非常直接。

越来越多的低代码开发平台把 AI agent 集成进来，让它们可以：

*   调用 API
    
*   修改记录
    
*   触发自动化流程
    

如果这些能力没有被严格约束，你很可能会得到一个“很好用、但潜在非常危险”的自动化系统。

所以，这个事件其实是在提醒所有平台：  
**AI 的能力必须嵌在一个有边界、有治理、有可观测性的架构里，而不是做成一个“随意接入的插件”。**

---

### Q3. 对一个开发平台来说，“AI-native” 到底意味着什么？

一个AI原生的开发平台，意味着它从第一天建平台时就假设：

> “AI agent 和模型就是系统的一部分，而不是外挂。”

这会带来几个关键特征：

*   平台在架构上直接支持 AI agent 和模型
    
*   工具链、安全机制、治理体系都天然地“懂 AI”
    
*   数据的访问、保留、删除，从设计阶段就被考虑进来，而不是上线后再补救
    

像 [JitAI](https://jit.pro) 这样的平台，就是沿着这个方向走：  
既做 **AI 原生架构**，又提供 low code development 的高效开发体验。

---

### Q4. 企业如果想安全地开始用 AI agent，有没有一个推荐路径？

一个相对安全靠谱的节奏可以是：

*   从 **范围小、风险低的场景** 入手，比如内部问答、文档助手、只读数据的分析等
    
*   使用真正 AI 原生的 development platform，要求它**支持类型化工具、最小权限和行为日志**
    
*   随着团队在治理和经验上的成熟，逐步放大 AI agent 的能力边界
    

如果你希望在一个更可控的环境里探索这些能力，可以先在沙箱里 [try JitAI](https://jit.pro/download)，看看一个 AI 原生平台是如何把 “速度” 和 “安全” 尽量兼顾起来的。

---

## 10. 小结：从一则新闻，到平台架构的思考

“Google AI data deletion” 这件事，不只是某个 AI 功能的翻车现场，更像是对整个行业的一句提醒：

*   如果你给了 AI 过高的权限，却没有设计好约束，迟早会发生危险的事
    
*   数据、操作和 AI 推理，不应该被割裂治理
    
*   下一代低代码开发平台和开发平台，如果想真正支撑企业级 AI，就必须是 **AI 原生** 的
    

如果你的组织已经在考虑从“AI 玩具”走向“AI 生产力”，那你需要问的，不再只是：

> “这个模型准不准？”

而是：

> “我们现在用的平台，是不是为 AI 留好了安全的轨道？”

像 [JitAI](https://jit.pro) 这样的 AI 原生平台，目标就是让你可以大胆去用 AI Agent 接业务系统，同时仍然保留：**可审计、可治理、可控制** 的底线。  
不是寄希望于 “AI 别出错”，而是通过架构设计，让它即使出错，也不会轻易酿成灾难。
